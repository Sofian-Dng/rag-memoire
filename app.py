import os
import shutil
import streamlit as st
from dotenv import load_dotenv
import nest_asyncio

# Permettre les boucles d'√©v√©nements imbriqu√©es pour Streamlit
nest_asyncio.apply()
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from openai import OpenAI

# Importer le text splitter selon la version disponible
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter
    except ImportError:
        from langchain_community.text_splitter import RecursiveCharacterTextSplitter

# Configuration des chemins
DATA_PATH = "data/"
FAISS_PATH = "faiss_index/"

# Charger les variables d'environnement
def load_env_file():
    """Charge le fichier .env depuis le r√©pertoire courant"""
    env_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.env')
    if os.path.exists(env_path):
        # Lire et nettoyer le fichier du BOM
        try:
            with open(env_path, 'r', encoding='utf-8-sig') as f:  # utf-8-sig supprime automatiquement le BOM
                content = f.read()
            
            # R√©√©crire le fichier sans BOM
            with open(env_path, 'w', encoding='utf-8') as f:
                f.write(content)
        except Exception:
            pass  # Si erreur, continuer quand m√™me
        
        load_dotenv(dotenv_path=env_path, override=True)
        return True
    return False

# Charger le .env au d√©marrage
load_env_file()

# Chargement des secrets (local .env ou Streamlit secrets pour d√©ploiement)
def get_secret(key):
    """R√©cup√®re une variable depuis .env ou st.secrets"""
    # Recharger le .env √† chaque fois pour √™tre s√ªr (seulement en mode d√©veloppement)
    if not hasattr(get_secret, '_env_loaded'):
        load_env_file()
        get_secret._env_loaded = True
    
    # Essayer d'abord avec os.getenv (pour d√©veloppement local avec .env)
    value = os.getenv(key)
    if value is not None and value.strip() != '':
        return value.strip()
    
    # Essayer aussi avec la cl√© nettoy√©e du BOM (au cas o√π le BOM serait dans le nom de la cl√©)
    # Le BOM UTF-8 est \ufeff
    if value is None:
        # Essayer de lire directement depuis le fichier .env
        env_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.env')
        if os.path.exists(env_path):
            try:
                with open(env_path, 'r', encoding='utf-8-sig') as f:
                    for line in f:
                        line = line.strip()
                        if not line or line.startswith('#'):
                            continue
                        if '=' in line:
                            env_key, env_value = line.split('=', 1)
                            # Nettoyer la cl√© du BOM et des espaces
                            env_key = env_key.strip().lstrip('\ufeff').strip()
                            if env_key == key:
                                return env_value.strip()
            except Exception:
                pass
    
    # Sinon essayer avec st.secrets (pour d√©ploiement)
    try:
        return st.secrets[key]
    except (AttributeError, KeyError):
        return None
    except Exception:
        # Capturer toutes les autres exceptions (comme StreamlitSecretNotFoundError)
        return None

# Fonction d'authentification
def authenticate():
    """Affiche un formulaire de connexion et v√©rifie les identifiants"""
    st.title("üîê Authentification")
    
    username = get_secret("USERNAME")
    password = get_secret("PASSWORD")
    
    # Nettoyer les valeurs (enlever les espaces)
    if username:
        username = username.strip()
    if password:
        password = password.strip()
    
    if username is None or password is None:
        st.error("‚ö†Ô∏è Les identifiants ne sont pas configur√©s. Veuillez d√©finir USERNAME et PASSWORD dans .env ou st.secrets")
        # Debug: afficher ce qui a √©t√© charg√©
        with st.expander("üîç Debug - Variables charg√©es"):
            st.write(f"USERNAME charg√©: {repr(username)}")
            st.write(f"PASSWORD charg√©: {repr(password)}")
            st.write(f"Fichier .env existe: {os.path.exists('.env')}")
        return False
    
    with st.form("login_form"):
        input_username = st.text_input("Nom d'utilisateur")
        input_password = st.text_input("Mot de passe", type="password")
        submit_button = st.form_submit_button("Se connecter")
        
        if submit_button:
            # Nettoyer aussi les entr√©es utilisateur
            input_username = input_username.strip()
            input_password = input_password.strip()
            
            # Debug en cas d'√©chec
            if input_username != username or input_password != password:
                st.error("‚ùå Identifiants incorrects")
                with st.expander("üîç Debug - Comparaison"):
                    st.write(f"Username attendu: {repr(username)}")
                    st.write(f"Username saisi: {repr(input_username)}")
                    st.write(f"Match username: {input_username == username}")
                    st.write(f"Password attendu: {repr('*' * len(password) if password else None)}")
                    st.write(f"Password saisi: {repr('*' * len(input_password) if input_password else None)}")
                    st.write(f"Match password: {input_password == password}")
                return False
            else:
                st.session_state.authenticated = True
                st.rerun()
    
    return st.session_state.get("authenticated", False)

# Fonction pour obtenir les embeddings (locaux, rapides)
def get_embeddings():
    """Retourne les embeddings locaux (Sentence Transformers) - beaucoup plus rapides que Gemini"""
    # Utiliser un mod√®le fran√ßais l√©ger et rapide
    model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    return HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={'device': 'cpu'},  # Utiliser CPU (plus compatible)
        encode_kwargs={'normalize_embeddings': True}
    )

# Fonction d'indexation des documents
def index_documents():
    """Indexe les documents PDF dans FAISS"""
    try:
        # V√©rifier si l'index existe d√©j√†
        if os.path.exists(FAISS_PATH) and os.path.exists(os.path.join(FAISS_PATH, "index.faiss")):
            st.info("üìö Chargement de l'index existant...")
            embeddings = get_embeddings()
            vector_store = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
            st.success("‚úÖ Index charg√© avec succ√®s !")
            return vector_store
        
        # Sinon, cr√©er un nouvel index
        st.info("üìö Cr√©ation d'un nouvel index...")
        
        # V√©rifier que le dossier data existe
        if not os.path.exists(DATA_PATH):
            st.error(f"‚ùå Le dossier {DATA_PATH} n'existe pas. Veuillez le cr√©er et y placer vos fichiers PDF.")
            return None
        
        # Charger les fichiers PDF
        pdf_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.pdf')]
        
        if not pdf_files:
            st.error(f"‚ùå Aucun fichier PDF trouv√© dans {DATA_PATH}")
            return None
        
        st.info(f"üìÑ {len(pdf_files)} fichier(s) PDF trouv√©(s). Indexation en cours...")
        
        # Charger et segmenter les documents
        documents = []
        for pdf_file in pdf_files:
            loader = PyPDFLoader(os.path.join(DATA_PATH, pdf_file))
            docs = loader.load()
            # Ajouter le nom du fichier comme m√©tadonn√©e
            for doc in docs:
                doc.metadata['source'] = pdf_file
            documents.extend(docs)
        
        # Segmenter les documents
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)
        
        st.info(f"üìù {len(splits)} segments cr√©√©s. G√©n√©ration des embeddings en cours...")
        st.info("üí° Utilisation d'embeddings locaux (rapides) - le mod√®le sera t√©l√©charg√© la premi√®re fois uniquement")
        
        # Utiliser les embeddings locaux (beaucoup plus rapides)
        with st.spinner("Chargement du mod√®le d'embeddings (premi√®re fois uniquement)..."):
            embeddings = get_embeddings()
        
        # Pour les petits documents (< 100 segments), traiter tout en une fois (plus rapide)
        # Pour les gros documents, utiliser des lots pour √©viter les timeouts
        if len(splits) < 100:
            # Traitement en une seule fois pour les petits documents
            with st.spinner("G√©n√©ration des embeddings (quelques secondes)..."):
                try:
                    vector_store = FAISS.from_documents(splits, embeddings)
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Erreur, nouvelle tentative...")
                    vector_store = FAISS.from_documents(splits, embeddings)
        else:
            # Traitement par lots pour les gros documents
            batch_size = 50  # Lots plus grands pour r√©duire le nombre d'appels
            total_batches = (len(splits) + batch_size - 1) // batch_size
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Cr√©er le vector store progressivement
            vector_store = None
            for i in range(0, len(splits), batch_size):
                batch = splits[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                
                status_text.text(f"Traitement du lot {batch_num}/{total_batches} ({len(batch)} segments)...")
                progress_bar.progress(batch_num / total_batches)
                
                try:
                    if vector_store is None:
                        # Premier lot : cr√©er le vector store
                        vector_store = FAISS.from_documents(batch, embeddings)
                    else:
                        # Lots suivants : ajouter au vector store existant
                        vector_store.add_documents(batch)
                except Exception as e:
                    # En cas d'erreur, r√©essayer une fois
                    st.warning(f"‚ö†Ô∏è Erreur sur le lot {batch_num}, nouvelle tentative...")
                    try:
                        if vector_store is None:
                            vector_store = FAISS.from_documents(batch, embeddings)
                        else:
                            vector_store.add_documents(batch)
                    except Exception as e2:
                        st.error(f"‚ùå Erreur persistante sur le lot {batch_num}: {str(e2)}")
                        raise e2
            
            progress_bar.empty()
            status_text.empty()
        
        # Sauvegarder l'index
        os.makedirs(FAISS_PATH, exist_ok=True)
        vector_store.save_local(FAISS_PATH)
        
        st.success(f"‚úÖ Index cr√©√© avec succ√®s ! {len(splits)} segments index√©s.")
        return vector_store
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors de l'indexation : {str(e)}")
        return None

# Fonction pour g√©n√©rer une r√©ponse (approche directe avec plus de contexte)
def generate_answer(vector_store, question, mode="question"):
    """G√©n√®re une r√©ponse d√©velopp√©e en utilisant plusieurs segments de documents
    
    Args:
        vector_store: Le vector store FAISS
        question: La question de l'utilisateur
        mode: "question" pour r√©ponses normales, "redaction" pour format m√©moire acad√©mique
    """
    
    # R√©cup√©rer plusieurs documents pertinents (10 segments pour maximum de contexte)
    docs = vector_store.similarity_search(question, k=10)
    
    if not docs:
        return "Aucun document pertinent trouv√©.", []
    
    # Construire le contexte √† partir de tous les documents trouv√©s
    context_parts = []
    sources_info = []
    
    for i, doc in enumerate(docs, 1):
        source_name = doc.metadata.get("source", "Inconnu")
        page = doc.metadata.get("page", "N/A")
        content = doc.page_content
        
        context_parts.append(f"[Document {i} - Source: {source_name}, Page: {page}]\n{content}")
        sources_info.append((source_name, page))
    
    # Combiner tous les contextes
    full_context = "\n\n---\n\n".join(context_parts)
    
    # Cr√©er le prompt selon le mode
    if mode == "redaction":
        # PROMPT MODE R√âDACTION - Format m√©moire acad√©mique ultra-pr√©cis
        prompt_text = f"""Vous √™tes un chercheur r√©digeant un m√©moire acad√©mique de niveau universitaire. Votre t√¢che est de r√©diger un texte qui pourra √™tre INT√âGR√â DIRECTEMENT dans un m√©moire, en respectant les normes acad√©miques les plus strictes.

FORMAT ET STYLE OBLIGATOIRES :
- R√©digez comme un chercheur universitaire : ton scientifique, pr√©cis, objectif
- Structurez en paragraphes coh√©rents et bien encha√Æn√©s
- Utilisez un vocabulaire acad√©mique et sp√©cialis√©
- √âvitez les formulations famili√®res ou conversationnelles
- Chaque paragraphe doit d√©velopper une id√©e principale
- Utilisez des transitions logiques entre les paragraphes
- Int√©grez les citations de mani√®re fluide dans le texte
- Format de citation : (Source: nom_fichier.pdf, Page: X)
- Longueur : minimum 4-6 paragraphes d√©velopp√©s
- Pas de formules de politesse, pas de "je", pas de "nous" sauf si n√©cessaire
- √âcrivez comme si c'√©tait d√©j√† dans votre m√©moire

STRUCTURE ATTENDUE :
1. Introduction du sujet (1-2 paragraphes)
2. D√©veloppement argument√© avec synth√®se des sources (2-4 paragraphes)
3. Conclusion/synth√®se (1 paragraphe)

CONTEXTE (plusieurs documents pertinents) :
{full_context}

SUJET/QUESTION: {question}

R√âDACTION ACAD√âMIQUE (texte pr√™t √† √™tre int√©gr√© dans un m√©moire) :"""
    
    else:
        # PROMPT MODE QUESTION - R√©ponses normales et conversationnelles
        prompt_text = f"""Vous √™tes un assistant expert en r√©daction de m√©moire acad√©mique. Votre r√¥le est de fournir des r√©ponses D√âVELOPP√âES, D√âTAILL√âES et ARGUMENT√âES bas√©es strictement sur le CONTEXTE fourni ci-dessous.

INSTRUCTIONS IMPORTANTES :
- Fournissez une r√©ponse COMPL√àTE et D√âVELOPP√âE (minimum 3-4 paragraphes si possible)
- Structurez votre r√©ponse de mani√®re claire et acad√©mique
- Utilisez TOUS les √©l√©ments pertinents du contexte fourni
- Citez vos sources entre parenth√®ses : (Source: nom_fichier.pdf, Page: X)
- Ton formel mais accessible
- Si plusieurs documents abordent le sujet, synth√©tisez-les de mani√®re coh√©rente
- D√©veloppez les concepts, donnez des exemples si disponibles dans le contexte
- Ne vous contentez pas d'une r√©ponse courte, EXPLIQUEZ et D√âVELOPPEZ

CONTEXTE (plusieurs documents pertinents) :
{full_context}

QUESTION: {question}

R√âPONSE D√âVELOPP√âE (minimum 3-4 paragraphes, bien structur√©e) :"""
    
    # Utiliser OpenAI directement (plus rapide et fiable, sans LangChain)
    try:
        client = OpenAI(api_key=get_secret("OPENAI_API_KEY"))
        
        # D√©terminer le message syst√®me selon le mode
        if mode == "redaction":
            system_content = "Vous √™tes un chercheur universitaire r√©digeant un m√©moire acad√©mique. Votre style doit √™tre scientifique, pr√©cis et pr√™t √† √™tre int√©gr√© directement dans un document acad√©mique."
        else:
            system_content = "Vous √™tes un assistant expert en r√©daction acad√©mique. Vous fournissez des r√©ponses d√©velopp√©es, d√©taill√©es et bien structur√©es."
        
        # Appel direct √† l'API OpenAI avec GPT-4 et plus de tokens pour des r√©ponses d√©velopp√©es
        response = client.chat.completions.create(
            model="gpt-4-turbo-preview",  # GPT-4 Turbo pour meilleure qualit√©
            messages=[
                {"role": "system", "content": system_content},
                {"role": "user", "content": prompt_text}
            ],
            temperature=0.3,  # L√©g√®rement augment√© pour plus de vari√©t√©
            max_tokens=3000,  # Augment√© √† 3000 tokens pour des r√©ponses tr√®s d√©velopp√©es
            timeout=90  # Timeout augment√© pour GPT-4 qui peut √™tre plus lent
        )
        
        answer = response.choices[0].message.content
        
    except Exception as e:
        answer = f"Erreur lors de la g√©n√©ration: {str(e)}"
    
    return answer, docs

# Fonction principale
def main():
    """Fonction principale de l'application"""
    
    # Initialiser l'√©tat de session
    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = None
    
    if "mode" not in st.session_state:
        st.session_state.mode = "question"  # Mode par d√©faut : question
    
    # V√©rifier l'authentification
    if not st.session_state.authenticated:
        if not authenticate():
            return
    
    # V√©rifier que l'API key est disponible
    api_key = get_secret("OPENAI_API_KEY")
    if not api_key:
        st.error("‚ö†Ô∏è OPENAI_API_KEY n'est pas configur√©e. Veuillez la d√©finir dans .env ou st.secrets")
        
        # Section de debug
        with st.expander("üîç Debug - Informations de chargement"):
            env_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.env')
            st.write(f"**Chemin du fichier .env :** `{env_path}`")
            st.write(f"**Fichier .env existe :** {os.path.exists(env_path)}")
            
            if os.path.exists(env_path):
                st.write("**Contenu du fichier .env (masqu√©) :**")
                try:
                    with open(env_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        for i, line in enumerate(lines, 1):
                            if '=' in line:
                                key, value = line.split('=', 1)
                                if 'PASSWORD' in key or 'API_KEY' in key:
                                    st.write(f"Ligne {i}: {key.strip()}=***")
                                else:
                                    st.write(f"Ligne {i}: {line.strip()}")
                except Exception as e:
                    st.write(f"Erreur lors de la lecture: {e}")
            
            st.write(f"**OPENAI_API_KEY charg√©e :** {bool(api_key)}")
            st.write(f"**USERNAME charg√© :** {bool(get_secret('USERNAME'))}")
            st.write(f"**PASSWORD charg√© :** {bool(get_secret('PASSWORD'))}")
            
            # Afficher toutes les variables d'environnement qui commencent par OPENAI, USERNAME ou PASSWORD
            st.write("**Variables d'environnement d√©tect√©es :**")
            env_vars = {k: v for k, v in os.environ.items() if any(x in k for x in ['OPENAI', 'USERNAME', 'PASSWORD'])}
            for k, v in env_vars.items():
                if 'PASSWORD' in k or 'API_KEY' in k:
                    st.write(f"- {k}: ***")
                else:
                    st.write(f"- {k}: {v}")
        
        return
    
    # Charger automatiquement l'index s'il existe et n'est pas d√©j√† charg√©
    if st.session_state.vector_store is None:
        if os.path.exists(FAISS_PATH) and os.path.exists(os.path.join(FAISS_PATH, "index.faiss")):
            try:
                embeddings = get_embeddings()
                st.session_state.vector_store = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
                # Stocker l'erreur de chargement si elle existe
                if "index_load_error" in st.session_state:
                    del st.session_state["index_load_error"]
            except Exception as e:
                # Stocker l'erreur pour l'afficher dans la sidebar
                st.session_state["index_load_error"] = str(e)
    
    # Barre lat√©rale
    with st.sidebar:
        st.title("ü§ñ Assistant RAG M√©moire")
        st.markdown("---")
        
        # S√©lecteur de mode (Question / R√©daction)
        st.subheader("üìù Mode de r√©ponse")
        mode_redaction = st.checkbox(
            "Mode R√©daction (format m√©moire acad√©mique)",
            value=(st.session_state.mode == "redaction"),
            help="Cochez cette case pour obtenir des r√©ponses format√©es comme un chercheur dans un m√©moire. D√©cochez pour des r√©ponses normales."
        )
        
        # Mettre √† jour le mode dans session_state
        if mode_redaction:
            st.session_state.mode = "redaction"
            st.info("‚úçÔ∏è Mode **R√©daction** activ√© : r√©ponses pr√™tes √† copier-coller dans votre m√©moire")
        else:
            st.session_state.mode = "question"
            st.info("‚ùì Mode **Question** activ√© : r√©ponses conversationnelles")
        
        st.markdown("---")
        
        # Afficher un message si l'index n'a pas pu √™tre charg√©
        if st.session_state.vector_store is None and os.path.exists(FAISS_PATH) and os.path.exists(os.path.join(FAISS_PATH, "index.faiss")):
            st.error("‚ùå Impossible de charger l'index existant")
            if "index_load_error" in st.session_state:
                with st.expander("üîç D√©tails de l'erreur"):
                    st.text(st.session_state["index_load_error"])
            if st.button("üóëÔ∏è Supprimer l'index corrompu"):
                if os.path.exists(FAISS_PATH):
                    shutil.rmtree(FAISS_PATH)
                if "index_load_error" in st.session_state:
                    del st.session_state["index_load_error"]
                st.rerun()
        
        # Bouton d'indexation
        if st.button("üìö Indexer les documents", type="primary"):
            with st.spinner("Indexation en cours..."):
                vector_store = index_documents()
                if vector_store:
                    st.session_state.vector_store = vector_store
                    st.rerun()
        
        # Afficher le statut de l'index
        if st.session_state.vector_store:
            st.success("‚úÖ Index disponible")
        else:
            st.warning("‚ö†Ô∏è Index non charg√©. Cliquez sur 'Indexer les documents'.")
            # Debug: v√©rifier si le dossier existe
            if os.path.exists(FAISS_PATH):
                if os.path.exists(os.path.join(FAISS_PATH, "index.faiss")):
                    st.info("‚ÑπÔ∏è Index FAISS trouv√© mais non charg√©. Essayez de r√©indexer.")
                else:
                    st.info("‚ÑπÔ∏è Dossier index vide. Indexation n√©cessaire.")
        
        st.markdown("---")
        
        # Bouton de d√©connexion
        if st.button("üö™ Se d√©connecter"):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    # Interface principale
    st.title("üí¨ Chatbot RAG - Assistant M√©moire")
    st.markdown("**Sujet :** Wokisme")
    st.markdown("---")
    
    # Plus besoin d'initialiser la cha√Æne QA, on utilise une approche directe
    
    # Afficher l'historique des messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            # Afficher les sources si disponibles
            if "sources" in message and message["sources"]:
                with st.expander("üìö Sources"):
                    for source in message["sources"]:
                        source_name = source.metadata.get("source", "Inconnu")
                        page = source.metadata.get("page", "N/A")
                        st.text(f"‚Ä¢ {source_name} (Page {page})")
    
    # Entr√©e utilisateur
    if prompt := st.chat_input("Posez votre question sur le wokisme..."):
        # V√©rifier que le vector store est disponible
        if st.session_state.vector_store is None:
            st.error("‚ö†Ô∏è Veuillez d'abord indexer les documents depuis la barre lat√©rale.")
            return
        
        # Ajouter le message utilisateur √† l'historique
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Afficher le message utilisateur
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # G√©n√©rer la r√©ponse (approche directe et rapide)
        with st.chat_message("assistant"):
            with st.spinner("R√©flexion en cours..."):
                try:
                    # Appel direct avec le mode s√©lectionn√©
                    answer, sources = generate_answer(
                        st.session_state.vector_store, 
                        prompt, 
                        mode=st.session_state.mode
                    )
                    
                    # Afficher la r√©ponse
                    st.markdown(answer)
                    
                    # Afficher les sources
                    if sources:
                        with st.expander("üìö Sources utilis√©es"):
                            unique_sources = {}
                            for source in sources:
                                source_name = source.metadata.get("source", "Inconnu")
                                page = source.metadata.get("page", "N/A")
                                key = f"{source_name}_{page}"
                                if key not in unique_sources:
                                    unique_sources[key] = (source_name, page)
                            
                            for source_name, page in unique_sources.values():
                                st.text(f"‚Ä¢ {source_name} (Page {page})")
                    
                    # Ajouter la r√©ponse √† l'historique
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer,
                        "sources": sources
                    })
                    
                except Exception as e:
                    error_msg = f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse : {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })

# Point d'entr√©e
if __name__ == "__main__":
    main()

